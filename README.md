# Triton Experiments

## Resources

- Tutorials: https://github.com/triton-inference-server/tutorials/tree/main
- HF to onnx: https://github.com/triton-inference-server/model_navigator/blob/main/examples/torch/bert/optimize.py#L63
- Python DLPack: https://dmlc.github.io/dlpack/latest/python_spec.html
- Custom backend stub and custom environment: https://developer.nvidia.com/blog/serving-ml-model-pipelines-on-nvidia-triton-inference-server-with-ensemble-models/
- NVIDIA developer forum: https://forums.developer.nvidia.com/tag/inference-server-triton
- https://github.com/triton-inference-server/pytriton
- Docs (2): https://docs.nvidia.com/deeplearning/triton-inference-server/archives/triton_inference_server_1140/user-guide/docs/models_and_schedulers.html